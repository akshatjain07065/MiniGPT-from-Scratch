# MiniGPT-from-Scratch
This repository contains a small-scale Transformer-based LLM built from scratch using PyTorch and Hugging Face. The model is trained on Wikipedia data and deployed via FastAPI.
# ðŸš€ MiniGPT - A Transformer-based Language Model from Scratch

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.8%2B-green.svg)](https://www.python.org/)

## ðŸ“œ About
MiniGPT is a lightweight Transformer-based language model, built entirely from scratch using PyTorch. It is trained on Wikipedia and demonstrates text generation capabilities similar to GPT.

## ðŸŽ¯ Features
âœ… Train a small-scale Transformer model  
âœ… Tokenize text using Byte-Pair Encoding (BPE)  
âœ… Implement Transformer layers from scratch  
âœ… Generate text from input prompts  
âœ… Deploy as an API using FastAPI  

## ðŸ“‚ Project Structure
ðŸ“‚ MiniGPT-from-Scratch/
â”‚â”€â”€ ðŸ“‚ data/               # Contains dataset and preprocessing scripts
â”‚â”€â”€ ðŸ“‚ models/             # Transformer model architecture
â”‚â”€â”€ ðŸ“‚ training/           # Training scripts
â”‚â”€â”€ ðŸ“‚ inference/          # Code for generating text
â”‚â”€â”€ ðŸ“‚ deployment/         # FastAPI deployment scripts
â”‚â”€â”€ ðŸ“œ README.md           # Detailed documentation
â”‚â”€â”€ ðŸ“œ requirements.txt    # List of dependencies
â”‚â”€â”€ ðŸ“œ train.py            # Main training script
â”‚â”€â”€ ðŸ“œ generate.py         # Script for text generation
â”‚â”€â”€ ðŸ“œ app.py              # FastAPI application
â”‚â”€â”€ ðŸ“œ notebook.ipynb      # Jupyter Notebook for Colab training
